[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Byers First Day Blog",
    "section": "",
    "text": "Ordered Beta Regression and Other Models\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nJ. Branson Byers\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is my attempt to make a new blog post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Ordered Regression/index.html",
    "href": "posts/Ordered Regression/index.html",
    "title": "Ordered Beta Regression and Other Models",
    "section": "",
    "text": "In psychology research, it is common to come across data sets that are comprised of continuous responses on a finite scale. When it comes time to making claims about that data, scientists often default to Ordinary Least Squares Regression. Let’s take a dataset of precisely this kind of data: continuous, bounded responses from humans, and see how different models perform. We will note some flaws of these models, and demonstrate the benefits of some of the newer, more advanced models being used today. We make the case that for any kind of bounded, continuous data, Ordered Beta Regression should be used since it is a more general and sparse model than other good alternatives.\n\n\nThe data that we examine here is an approximated version of an unpublished data set regarding disagreement. Let’s load in our data, and take a look at what we are working with.\n\n# read in the data\ndata = read.csv(file = 'ord_bet_reg_data.csv')\n\n# make a simple plot the depicts what we are interested in\ndisagreement_figure = ggplot(data, aes(x=divergence, y=disagreement)) + \n  theme_classic(base_size = 15) + \n  geom_point(alpha=1, size=3, color='cadetblue') + \n  labs(y = 'Disagreement', x = 'Difference in beliefs') +\n  scale_fill_brewer(palette = 'Dark2') +\n  #facet_wrap(~ importance_binarized) +\n  ggtitle('Actual data')\n\ndisagreement_figure\n\n\n\n\n\n\n\n\nWhat we see here is a roughly positive trend in the data, with a decent amount of data appearing at the bounds of zero and one. Is there a way we could quantify both this positive relationship the capture how the data gathers at the bounds?"
  },
  {
    "objectID": "test_python.html",
    "href": "test_python.html",
    "title": "temp_python.ipynb",
    "section": "",
    "text": "library(reticulate)\n\npy_config()\n\nprint('I am alive!!')\n\nI am alive!!"
  },
  {
    "objectID": "posts/Ordered Regression/index.html#the-data",
    "href": "posts/Ordered Regression/index.html#the-data",
    "title": "Ordered Regression and Other Models",
    "section": "The Data",
    "text": "The Data\nThe data that we examine here is an approximated version of an unpublished data set regarding disagreement. Let’s load in our data, and take a look at what we are working with.\n\n# read in the data\ndata = read.csv(file = 'ord_bet_reg_data.csv')\n\n# make a simple plot the depicts what we are interested in\ndisagreement_figure = ggplot(data, aes(x=divergence, y=disagreement)) + \n  theme_classic(base_size = 15) + \n  geom_point(alpha=1, size=3, color='cadetblue') + \n  labs(y = 'Disagreement', x = 'Difference in beliefs') +\n  scale_fill_brewer(palette = 'Dark2') +\n  #facet_wrap(~ importance_binarized) +\n  ggtitle('Actual data')\n\ndisagreement_figure\n\n\n\n\n\n\n\n\nWhat we see here is a roughly positive trend in the data, with a decent amount of data appearing at the bounds of zero and one. Is there a way we could quantify both this positive relationship the capture how the data gathers at the bounds?"
  },
  {
    "objectID": "posts/Ordered Regression/index.html#linear-model",
    "href": "posts/Ordered Regression/index.html#linear-model",
    "title": "Ordered Beta Regression and Other Models",
    "section": "Linear Model",
    "text": "Linear Model\nIt is common for psychologists to use frequentist models in their work. As of late, many are turning to Bayesian approaches. One large upside of using a Bayesian approach is that one can get an estimate of how certain the estimates within one’s model are. Another upside, is that Bayesian methods provide a robust way conducting model comparison, which can offer a rigorous way of testing hypotheses about the form one one’s data.\n\nModel SpecificationModel ConvergenceResultsOther DiagnosticsPosterior Predictive Check\n\n\nFor now, we don’t go into the details about each of the regressors. What matters here is whether or not a linear model can capture the features of the data that we care about.\n\n# linear model\noutput = capture.output(linear_model <- brm(\n  formula = disagreement ~ divergence + change + confidence + importance + belief_strength + same_side,\n  data = data,\n  chains = 6, iter = 3000, warmup = 2000,\n  cores = 6, seed = 1234, \n  backend = \"cmdstanr\"\n))\n\n# we can add the `loo` criterion for model comparison later\nlinear_model = add_criterion(linear_model, 'loo',\"waic\")\n\n\n\nWe can see that our parameters are relatively smoothly Gaussian, and that the chains are intermingled like a “fuzzy caterpillar.” This means that our software is converging, which is good.\n\nplot(linear_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn case one is curious about the exact numerical estimates within the model. These are not of concern to us right now.\n\nkable(tidy(linear_model))\n\n\n\n \n  \n    effect \n    component \n    group \n    term \n    estimate \n    std.error \n    conf.low \n    conf.high \n  \n \n\n  \n    fixed \n    cond \n    NA \n    (Intercept) \n    0.42 \n    0.05 \n    0.31 \n    0.52 \n  \n  \n    fixed \n    cond \n    NA \n    divergence \n    0.72 \n    0.06 \n    0.61 \n    0.84 \n  \n  \n    fixed \n    cond \n    NA \n    change \n    -0.01 \n    0.04 \n    -0.08 \n    0.05 \n  \n  \n    fixed \n    cond \n    NA \n    confidence \n    -0.01 \n    0.05 \n    -0.10 \n    0.09 \n  \n  \n    fixed \n    cond \n    NA \n    importance \n    0.02 \n    0.04 \n    -0.05 \n    0.10 \n  \n  \n    fixed \n    cond \n    NA \n    belief_strength \n    -0.24 \n    0.07 \n    -0.38 \n    -0.11 \n  \n  \n    fixed \n    cond \n    NA \n    same_sideTRUE \n    -0.15 \n    0.03 \n    -0.21 \n    -0.09 \n  \n  \n    ran_pars \n    cond \n    Residual \n    sd__Observation \n    0.21 \n    0.01 \n    0.20 \n    0.23 \n  \n\n\n\n\n\n\n\nRhat should be exactly 1, which it is. Another indicator that our model has converged.\n\n# check the Rhat values\nkable(diagnostic_posterior(linear_model), digits=3)\n\n\n\n \n  \n    Parameter \n    Rhat \n    ESS \n    MCSE \n  \n \n\n  \n    b_belief_strength \n    1 \n    4924 \n    0.001 \n  \n  \n    b_change \n    1 \n    5911 \n    0.000 \n  \n  \n    b_confidence \n    1 \n    4945 \n    0.001 \n  \n  \n    b_divergence \n    1 \n    3807 \n    0.001 \n  \n  \n    b_importance \n    1 \n    5765 \n    0.000 \n  \n  \n    b_Intercept \n    1 \n    4942 \n    0.001 \n  \n  \n    b_same_sideTRUE \n    1 \n    4200 \n    0.000 \n  \n\n\n\n\n\n\n\nWe will discuss this more below, but it looks like there is some kind of systematic error between the predictions that our model is making and the actual data. Why might this be?\n\npp_check(linear_model)\n## Using 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\n\n\nAfter clicking through the tabs above, we can see that our model did indeed converge, and is giving us estimates for how each predictor influences disagreement judgements. But before we go through the work of interpreting these results, we ask ourselves, are we even using the right model? If we look at our posterior predictive checks above, we can see that something is amiss.\nWe can get a more intuitive sense of what is going wrong by using the samples generated from our MCMC sampler to simulate data. This allows us to visualize our data how our model sees our data.\n\nData Simulation\n\n# generate a data frame of new predicted values from the data that we already have. Does it even do well?\nlinear_predictions = predicted_draws(linear_model, newdata=data)\n\n# there are going to be an enormous number of these predictions, so let's thin them out randomly the idea is that we should still get proportional results to the full set\nlinear_predictions_subset = linear_predictions[sample(nrow(linear_predictions),500),]\n\n# plot the predicted data\n# Plot disagreement vs Belief Divergence\nlinear_predict_plot = ggplot(linear_predictions_subset, aes(x=divergence, y=.prediction)) + \n  theme_classic(base_size = 15) + \n  geom_point(alpha=1, size=3, color=\"pink\") + \n  labs(y = 'Disagreement', x = 'Belief Difference') +\n  scale_fill_brewer(palette = 'Dark2') +\n  ggtitle('Linear Simulation')\n\nWe can compare the simulated data to the actual data.\n\nplot_grid(disagreement_figure, linear_predict_plot)\n\n\n\n\n\n\n\n\nWe can immediately identify some issues, but we can also glean some insights. The first large issue is that our model makes predictions that are outside the actual possible range in our data set. That it, people can only give disagreement rating between 0 and 1, and our model predicts that they might respond more or less than those upper and lower bounds. Our model also appears unable to capture the clustering at the bounds that we see in our actual data.\nWhat this tells us, is that this clustering effect at the bounds of disagreement ratings is not just due to how our data is sampled. That is, this clustering at the bounds actually tells us something about the functional form people might be using to make judgement about disagreement. Let’s try another class of model, to see if it is able to better capture the interesting trends in our data."
  },
  {
    "objectID": "posts/Ordered Regression/index.html#beta-regression",
    "href": "posts/Ordered Regression/index.html#beta-regression",
    "title": "Ordered Beta Regression and Other Models",
    "section": "Beta Regression",
    "text": "Beta Regression\nIn order to improve our model’s capability of representing the data and its actual bounds, we turn to beta regression. The beta distribution is incredibly flexible since is able to take on many different shapes with different amounts of variance. It is also bounded between zero and one. However, this boundedness comes with a large asterisk. The beta distribution is bounded between zero and one exclusive of zero and one. That means that we cannot accurately model data at the bounds without making some kind of modification to the data. And these might be exactly the points that we care about! Perhaps our research question asks how willing people are to completely agree or disagree.\nSince we are specifically interested in this behavior at the bounds of our data, we already know the beta distribution will be unable to actually model the phenomenon that we are interested in. Below, we slightly squeeze our data to allow us to run this model in the first place. We take a look at what kind of predictions this model makes, because we will build upon it to make improved models.\n\nModel SpecificationModel ConvergenceResultsOther DiagnosticsPosterior Predictive Check\n\n\n\n\n# beta regression can't handle 0's or 1's\nfake_altered_data = data %>% \n  mutate(disagreement = ifelse(disagreement == 0, 0.001, disagreement)) %>%\n  mutate(disagreement = ifelse(disagreement == 1, 0.999, disagreement))\n\n# beta regression, which can only run on fake data\noutput = capture.output(beta_model <- brm(\n  bf(disagreement ~ divergence + change + confidence + importance + belief_strength + same_side,\n     phi ~ divergence + change + confidence + importance + belief_strength + same_side),\n  data = fake_altered_data,\n  family = Beta(),\n  chains = 6, iter = 3000, warmup = 2000,\n  cores = 6, seed = 1234, \n  backend = \"cmdstanr\"\n))\n\n# we can add the `loo` criterion for model fitting later\nbeta_model = add_criterion(beta_model, 'loo', 'waic')\n\n\n\nLooks like our model converges just fine.\n\nplot(beta_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor those who might like to peek at this.\n\nkable(tidy(beta_model))\n\n\n\n \n  \n    effect \n    component \n    group \n    term \n    estimate \n    std.error \n    conf.low \n    conf.high \n  \n \n\n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.15 \n    0.29 \n    -0.71 \n    0.43 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.14 \n    0.30 \n    -0.75 \n    0.44 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    3.05 \n    0.33 \n    2.41 \n    3.69 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.17 \n    0.18 \n    -0.52 \n    0.19 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.10 \n    0.26 \n    -0.61 \n    0.42 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.01 \n    0.18 \n    -0.36 \n    0.37 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.81 \n    0.36 \n    -1.50 \n    -0.10 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.64 \n    0.15 \n    -0.94 \n    -0.34 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    0.26 \n    0.35 \n    -0.43 \n    0.94 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    0.25 \n    0.20 \n    -0.14 \n    0.63 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    0.77 \n    0.26 \n    0.27 \n    1.29 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    -0.34 \n    0.18 \n    -0.68 \n    0.01 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    0.86 \n    0.37 \n    0.13 \n    1.60 \n  \n  \n    fixed \n    cond \n    NULL \n    NULL \n    0.16 \n    0.16 \n    -0.14 \n    0.47 \n  \n\n\n\n\n\n\n\nConvergence confirmed.\n\n# check the Rhat values\nkable(diagnostic_posterior(beta_model), digits=3)\n\n\n\n \n  \n    Parameter \n    Rhat \n    ESS \n    MCSE \n  \n \n\n  \n    b_belief_strength \n    1 \n    7598 \n    0.004 \n  \n  \n    b_change \n    1 \n    7786 \n    0.002 \n  \n  \n    b_confidence \n    1 \n    7475 \n    0.003 \n  \n  \n    b_divergence \n    1 \n    5949 \n    0.004 \n  \n  \n    b_importance \n    1 \n    7937 \n    0.002 \n  \n  \n    b_Intercept \n    1 \n    7070 \n    0.003 \n  \n  \n    b_phi_belief_strength \n    1 \n    6784 \n    0.005 \n  \n  \n    b_phi_change \n    1 \n    7828 \n    0.002 \n  \n  \n    b_phi_confidence \n    1 \n    7515 \n    0.003 \n  \n  \n    b_phi_divergence \n    1 \n    6298 \n    0.004 \n  \n  \n    b_phi_importance \n    1 \n    8394 \n    0.002 \n  \n  \n    b_phi_Intercept \n    1 \n    7107 \n    0.004 \n  \n  \n    b_phi_same_sideTRUE \n    1 \n    6659 \n    0.002 \n  \n  \n    b_same_sideTRUE \n    1 \n    6437 \n    0.002 \n  \n\n\n\n\n\n\n\nWe are generally getting the shape right here.\n\npp_check(beta_model)\n## Using 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\n\n\n\nData Simulation\n\n# generate a data frame of new predicted values from the data that we already have. does it even do well?\nbeta_predictions = predicted_draws(beta_model, newdata=data)\n\n# there are going to be an enormous number of these predictions, so let's thin them out randomly the idea is that we should still get proportional results to the full set\nbeta_predictions_subset = beta_predictions[sample(nrow(beta_predictions),500),]\n\n# plot the predicted data\n# Plot disagreement vs Belief Divergence\nbeta_predict_plot = ggplot(beta_predictions_subset, aes(x=divergence, y=.prediction)) + \n  theme_classic(base_size = 15) + \n  geom_point(alpha=1, size=3,  color=\"pink\") + \n  labs(y = 'Disagreement', x = 'Belief Difference') +\n  scale_fill_brewer(palette = 'Dark2') +\n  ggtitle('Beta Regression Simulation')\n\n\nplot_grid(disagreement_figure, beta_predict_plot)\n\n\n\n\n\n\n\n\nInterestingly, a Beta Distribution is able to capture our data quite well. We see a degree of clustering at the upper and lower ends of the belief difference spectrum, though it does not hit the bounds as we well know. A consequence of this we can see is that our simulated data is noisier than our original data. The Beta Distribution alone is not able to understand that a phenomenon at the bounds might be different than one not at the bounds. Let’s move to a model that takes these two different processes into account."
  },
  {
    "objectID": "posts/Ordered Regression/index.html#zero-one-inflated-beta-regression",
    "href": "posts/Ordered Regression/index.html#zero-one-inflated-beta-regression",
    "title": "Ordered Beta Regression and Other Models",
    "section": "Zero-One Inflated Beta Regression",
    "text": "Zero-One Inflated Beta Regression\nZero-One Inflated Beta Regression (ZOIB) is essentially a logistic regression and a beta regression meshed together. One parameter determines the likelihood that data will be generated by the logistic regression or the beta regression. The other parameters dictate the form of the Beta and Logistic regression separately. As a result of this, this model has many parameters. We do not show it here, but our model struggles to converge if we regress all of our predictors onto each parameter.\nThis is the price we pay for a more complex model. We see how well this model might perform, and then will ask, is there a way of achieving all the properties that we want in a model without introducing so many new parameters?\n\nModel SpecificationModel ConvergenceResultsOther DiagnosticsPosterior Predictive Check\n\n\n\n\n# zoib model\noutput = capture.output(zoib_model <- brm(\n  bf(\n    disagreement ~ divergence + change + confidence + importance + belief_strength + same_side,\n    phi ~ divergence + change + confidence + importance + belief_strength + same_side,\n    zoi ~ 1,\n    coi ~ 1\n     ),\n  data = data,\n  family = zero_one_inflated_beta(),\n  chains = 6, iter = 3000, warmup = 2000,\n  cores = 6, seed = 1234, \n  backend = \"cmdstanr\"\n))\n\n# we can add the `loo` criterion for model fitting later\nzoib_model = add_criterion(zoib_model, 'loo', 'waic')\n\n\n\nDespite having so many parameters, our model does indeed converge. This is likely because we have a relatively large data set of 500 samples.\n\nplot(zoib_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(zoib_model)\n##  Family: zero_one_inflated_beta \n##   Links: mu = logit; phi = log; zoi = logit; coi = logit \n## Formula: disagreement ~ divergence + change + confidence + importance + belief_strength + same_side \n##          phi ~ divergence + change + confidence + importance + belief_strength + same_side\n##          zoi ~ 1\n##          coi ~ 1\n##    Data: data (Number of observations: 500) \n##   Draws: 6 chains, each with iter = 3000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 6000\n## \n## Population-Level Effects: \n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              -0.58      0.23    -1.03    -0.14 1.00     7328     5145\n## phi_Intercept           1.84      0.36     1.15     2.56 1.00     6629     5038\n## zoi_Intercept          -1.79      0.13    -2.05    -1.55 1.00     8754     4624\n## coi_Intercept           1.14      0.27     0.63     1.68 1.00     8592     4224\n## divergence              3.24      0.27     2.71     3.79 1.00     6651     4807\n## change                 -0.10      0.15    -0.40     0.21 1.00     7529     4979\n## confidence             -0.13      0.22    -0.57     0.29 1.00     7027     4715\n## importance              0.40      0.17     0.06     0.73 1.00     7141     4721\n## belief_strength        -1.07      0.30    -1.66    -0.50 1.00     7073     4826\n## same_sideTRUE          -0.53      0.12    -0.77    -0.29 1.00     6547     4844\n## phi_divergence         -0.31      0.39    -1.06     0.46 1.00     6187     4773\n## phi_change              0.05      0.23    -0.41     0.51 1.00     7651     4788\n## phi_confidence         -0.24      0.33    -0.89     0.40 1.00     6666     4717\n## phi_importance          0.04      0.23    -0.42     0.49 1.00     6724     4796\n## phi_belief_strength     0.59      0.46    -0.31     1.47 1.00     7428     4799\n## phi_same_sideTRUE      -0.20      0.18    -0.55     0.16 1.00     6405     4765\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nChecking for convergence and high effective sample size with a more complex model. ESS is well above 1000 for all parameters.\n\n# check the Rhat values\nkable(diagnostic_posterior(zoib_model), digits=3)\n\n\n\n \n  \n    Parameter \n    Rhat \n    ESS \n    MCSE \n  \n \n\n  \n    b_belief_strength \n    1 \n    7037 \n    0.004 \n  \n  \n    b_change \n    1 \n    7488 \n    0.002 \n  \n  \n    b_coi_Intercept \n    1 \n    8480 \n    0.003 \n  \n  \n    b_confidence \n    1 \n    7003 \n    0.003 \n  \n  \n    b_divergence \n    1 \n    6638 \n    0.003 \n  \n  \n    b_importance \n    1 \n    7110 \n    0.002 \n  \n  \n    b_Intercept \n    1 \n    7320 \n    0.003 \n  \n  \n    b_phi_belief_strength \n    1 \n    7373 \n    0.005 \n  \n  \n    b_phi_change \n    1 \n    7528 \n    0.003 \n  \n  \n    b_phi_confidence \n    1 \n    6591 \n    0.004 \n  \n  \n    b_phi_divergence \n    1 \n    6146 \n    0.005 \n  \n  \n    b_phi_importance \n    1 \n    6718 \n    0.003 \n  \n  \n    b_phi_Intercept \n    1 \n    6587 \n    0.004 \n  \n  \n    b_phi_same_sideTRUE \n    1 \n    6387 \n    0.002 \n  \n  \n    b_same_sideTRUE \n    1 \n    6469 \n    0.002 \n  \n  \n    b_zoi_Intercept \n    1 \n    8671 \n    0.001 \n  \n\n\n\n\n\n\n\nWe are gaining some predictive power on the right side here in comparison to Beta Regression. Let’s simulate data to see why we might be achieving these gains.\n\npp_check(zoib_model)\n## Using 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\n\n\n\nData Simulation\n\n# generate a data frame of new predicted values from the data that we already have. does it even do well?\nzoib_predictions = predicted_draws(zoib_model, newdata=data)\n\n# there are going to be an enormous number of these predictions, so let's thin them out randomly the idea is that we should still get proportional results to the full set\nzoib_predictions_subset = zoib_predictions[sample(nrow(zoib_predictions),500),]\n\n# plot the predicted data\n# Plot disagreement vs Belief Divergence\nzoib_predict_plot = ggplot(zoib_predictions_subset, aes(x=divergence, y=.prediction)) + \n  theme_classic(base_size = 15) + \n  geom_point(alpha=1, size=3,  color=\"pink\") + \n  labs(y = 'Disagreement', x = 'Belief Difference') +\n  scale_fill_brewer(palette = 'Dark2') +\n  ggtitle('ZOIB Simulation')\n\n\nplot_grid(disagreement_figure, zoib_predict_plot)\n\n\n\n\n\n\n\n\nAdmittedly this looks great. We are capturing the effects that we wanted at the bounds and elsewhere. Our models are converging, but as we noted, it is difficult for them to converge if all regressors are included. This is likely because our model is two models squashed together. Do we really need two models made into one to get the effect that we want?"
  },
  {
    "objectID": "posts/Ordered Regression/index.html#ordered-beta-regression",
    "href": "posts/Ordered Regression/index.html#ordered-beta-regression",
    "title": "Ordered Beta Regression and Other Models",
    "section": "Ordered Beta Regression",
    "text": "Ordered Beta Regression\nWe suggest that Ordered Beta Regression is able to solve the problems with ZOIB. It is able to capture phenomena at the bounds of data without the number of parameters in the model exploding. It does this by introducing dependency between the likeliness of a data point being at the bounds of the data and how likely data points are to appear at the bounds as the dependent variable gets larger. This is done through two parameters in addition to the ones in a beta distribution referred to as cut points.\nThe analogy often used for understanding cut points is that in reality the phenomenon of interest is actually unbounded (like disagreement), but we cannot measure an unbounded phenomenon. We assume that if someone’s preference is far above the bounds, they become increasingly likely to give a response at the upper bound (if it is above) and at the lower bound (if it is below).\nThe cuts down on the number of parameters needed to model the same kind of data as a ZOIB model.\n\nModel SpecificationModel ConvergenceResultsOther DiagnosticsPosterior Predictive Check\n\n\n\n\n# apologies for the long output. attempts to hide were unsuccessful\nordered_beta_reg = ordbetareg(\n  formula = disagreement ~ divergence + belief_strength + importance + confidence + change + same_side,\n  data = data,\n  chains = 6, iter = 3000, warmup = 2000,\n  cores = 6, seed = 1234, \n)\n## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\n## clang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\n## In file included from <built-in>:1:\n## In file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\n## In file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n## In file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:88:\n## /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\n## namespace Eigen {\n## ^\n## /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\n## namespace Eigen {\n##                ^\n##                ;\n## In file included from <built-in>:1:\n## In file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:\n## In file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n## /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n## #include <complex>\n##          ^~~~~~~~~\n## 3 errors generated.\n## make: *** [foo.o] Error 1\n\n# we can add the `loo` criterion for model fitting later\nordered_beta_reg = add_criterion(ordered_beta_reg, 'loo', 'waic')\n\n\n\n\nplot(ordered_beta_reg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(ordered_beta_reg)\n##  Family: ord_beta_reg \n##   Links: mu = identity; phi = identity; cutzero = identity; cutone = identity \n## Formula: disagreement ~ divergence + belief_strength + importance + confidence + change + same_side \n##    Data: data (Number of observations: 500) \n##   Draws: 6 chains, each with iter = 3000; warmup = 2000; thin = 1;\n##          total post-warmup draws = 6000\n## \n## Population-Level Effects: \n##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept          -0.50      0.21    -0.92    -0.07 1.00     6319     4412\n## divergence          3.20      0.25     2.72     3.70 1.00     5671     4328\n## belief_strength    -1.02      0.27    -1.55    -0.48 1.00     7180     4966\n## importance          0.34      0.15     0.03     0.64 1.00     7563     4466\n## confidence         -0.18      0.20    -0.57     0.23 1.00     6841     4138\n## change             -0.15      0.14    -0.43     0.13 1.00     6908     3934\n## same_sideTRUE      -0.53      0.11    -0.75    -0.31 1.00     6451     4430\n## \n## Family Specific Parameters: \n##         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## phi         5.33      0.35     4.67     6.04 1.00     7875     4671\n## cutzero    -3.59      0.25    -4.09    -3.11 1.00     6054     4119\n## cutone      1.83      0.05     1.73     1.92 1.00     6042     4542\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n# check the Rhat values\nkable(diagnostic_posterior(ordered_beta_reg), digits=3)\n\n\n\n \n  \n    Parameter \n    Rhat \n    ESS \n    MCSE \n  \n \n\n  \n    b_belief_strength \n    1 \n    7123 \n    0.003 \n  \n  \n    b_change \n    1 \n    6785 \n    0.002 \n  \n  \n    b_confidence \n    1 \n    6883 \n    0.002 \n  \n  \n    b_divergence \n    1 \n    5616 \n    0.003 \n  \n  \n    b_importance \n    1 \n    7495 \n    0.002 \n  \n  \n    b_Intercept \n    1 \n    6282 \n    0.003 \n  \n  \n    b_same_sideTRUE \n    1 \n    6443 \n    0.001 \n  \n\n\n\n\n\n\n\nWe nicely follow the predicted shape here.\n\npp_check(ordered_beta_reg)\n## Using 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\n\n\n\nData Simulation\n\n# generate a data frame of new predicted values from the data that we already have. does it even do well?\nord_beta_predictions = predicted_draws(ordered_beta_reg, newdata=data)\n\n# there are going to be an enormous number of these predictions, so let's thin them out randomly the idea is that we should still get proportional results to the full set\nord_beta_predictions_subset = ord_beta_predictions[sample(nrow(ord_beta_predictions),500),]\n\n# plot the predicted data\n# Plot disagreement vs Belief Divergence\nord_beta_predict_plot = ggplot(ord_beta_predictions_subset, aes(x=divergence, y=.prediction)) + \n  theme_classic(base_size = 15) + \n  geom_point(alpha=1, size=3,  color=\"pink\") + \n  labs(y = 'Disagreement', x = 'Belief Difference') +\n  scale_fill_brewer(palette = 'Dark2') +\n  ggtitle('Ordered Beta Simulation')\n\nRecall that our actual data looks like this.\n\nplot_grid(disagreement_figure, ord_beta_predict_plot)\n\n\n\n\n\n\n\n\nVoila! A lower parameter method for capturing bounded data with phenomena at the bounds."
  },
  {
    "objectID": "posts/Ordered Regression/index.html#ordered-beta-regression-1",
    "href": "posts/Ordered Regression/index.html#ordered-beta-regression-1",
    "title": "Ordered Regression and Other Models",
    "section": "Ordered Beta Regression",
    "text": "Ordered Beta Regression"
  }
]